### アンサンブル学習（ensemble learning）
複数のモデルを組み合わせて予測する。
具体的にはバギング（bagging）とブースティング（boosting）二種類があります。それぞれ代表的なアルゴリズムとしてランダムフォレスト（Random Forest）と勾配ブースティング（Gradient Boosting）
またバギングとブ個別のアルゴリズムのチューニングでは突破できない性能を、アンサンブル学習では実現できる可能性があります。精度追求が重要な局面においてアンサンブル学習は重要な一つのオプションとなり得ます。

バギング（bootstrap aggregating: bagging）は、まず元の訓練データ（n行）からランダムにn行のデータを復元抽出（重複を許して抽出）し、新しい訓練データを作成するということを繰り返します（ブートストラップと言います）。そして、その各標本に対して一つ一つモデルを作成し、モデルの結果を集約して予測をします。結果の集約は分類であれば多数決、回帰であれば平均値を取るなどします。元の訓練データと少しずつ異なる訓練データに対してモデルが構築されるので、モデルが過学習傾向にある時、バギングによって汎化性能を向上させられる可能性があります。
ブースティング（boosting）は、訓練データもモデルも逐次的に生成・構築されていきます。まずオリジナルの訓練データに対し最初のモデルが構築されます。この時点で予測と正解を比較して合致しているサンプル、外しているサンプルを把握します。そして外したサンプルが、次のモデリング段階で重視されるように新しい訓練データが生成されます。このようなステップが繰り返される過程でモデルも逐次的に複数構築されるのです。最後に、それらの予測値を組み合わせることで汎化性能の向上が図られます。


